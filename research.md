---
layout: default
title: Research
mathjax: true
---

# Research

## High-dimensional inference on graphical models

The 'high-dimensional data' revolution has left almost no field untouched. In recent years, due to advances in science and technology, high-throughput data from genomics, finance, environmental and marketing (among other) applications is being generated at a rapid pace. This has created an urgent need for methodology and tools for analyzing high-dimensional data. In biological applications, gene expression data where the number of genes is in the tens of thousands, and the number of available samples is in the hundreds or less is very common. In the environmental and climate sciences, measurements are often recorded at numerous locations on many variables. Formulating correct statistical models that make sense of all the many complex relationships and multivariate dependencies that are in the data, investigating the properties of these models and developing inferential procedures has provided major challenges for statisticians, probabilists, and others working in this field.

A class of models that are well-suited for high-dimensional inference, and have found widespread applications, are 'graphical models'. More specifically, a graphical model is a probability model that characterizes the marginal or conditional independence structure of a set of random variables by a graph. For Gaussian distributions, marginal and conditional independencies translate respectively into zeros in the covariance matrix and the inverse covariance matrix. Graphical models therefore have a natural place in high-dimensional inference since they provide a natural mechanism to reduce the parameter space, a fundamental step in modeling complex systems where the number of random variables often exceed the number of observations (the 'large $p$ small $n$' problem).

A subclass of graphical models that has received recent attention in the literature are Gaussian DAG models. In particular, suppose we have i.i.d. observations $Y_1, Y_2, \cdots, Y_n$ from a $p$-variate normal distribution with mean vector $0$ and covariance matrix $\Sigma$. Let $\Omega = LD^{-1}L^T$ be the modified Cholesky decomposition of the inverse covariance matrix $\Omega = \Sigma^{-1}$, i.e., $L$ is a lower triangular matrix with unit diagonal entries, and $D$ is a diagonal matrix with positive diagonal entries. For a DAG model, this normal distribution is assumed to be Markov with respect to a given directed acyclic graph $\mathscr{D}$ with vertices $\{1,2, \cdots, p\}$ (edges directed from larger to smaller vertices). This is equivalent to saying that $L_{ij}  = 0$ whenever $\mathscr{D}$ does not have a directed edge from $i$ to $j$.  Hence a Gaussian DAG model restricts $\Sigma$ (and $\Omega$) to a lower dimensional space by imposing sparsity constraints encoded in $\mathscr{D}$ on $L$.

Recently, Ben-David et al. introduce a class of DAG-Wishart distributions with multiple shape parameters. This class of distributions is defined for arbitrary DAG models. In particular, we consider a hierarchical Gaussian DAG model with DAG-Wishart priors on the covariance matrix and independent Bernoulli priors for each edge in the DAG. Under standard regularity assumptions, we show that under the true model, the posterior probability of the true graph converges in probability to $1$ as $n \rightarrow \infty$, and we develop a computationally feasible approach which searches around the graphs on the penalized likelihood solution path by adding or removing edges and choose the graph with the maximum posterior probability. This Bayesian procedure maintains the advantage of being able to do a principled broader search (for improved accuracy) in a computationally feasible way. We demonstrate the improvement in graph selection performance that can be obtained as compared to penalized likelihood approaches. We have had extremely positive feedback on our work from leaders in the field.

## High-dimensional model selection in regression

The literature on Bayesian variable selection in linear regression is vast and rich. Many priors and methods have been proposed. In recent years, the use of non-local priors in this context has generated a lot of interest. 

Non-local priors were first introduced by Johnson and Rossell as densities that are identically zero whenever a model parameter is equal to its null value in the context of hypothesis testing. In particular, let $y_n$ denote a random vector of responses, $X_n$ an $n \times p$ design matrix of covariates, and $\beta = (\beta_1, \beta_2, \ldots, \beta_p)$ a $p \times 1$ vector of regression coefficients. Under the linear regression model, 

$$y_n \sim N\left(X_n\beta, \sigma^2I_n\right).$$

Later, they introduced the product moment (pMOM) non-local prior with density

$$d_p(2\pi)^{-\frac p 2}(\tau \sigma^2)^{-rp - \frac p 2} |A_p| ^{\frac 1 2} \exp \left\{- \frac{\beta^\prime A_p \beta}{2 \tau \sigma ^2}\right\}\prod_{i =1}^{p} \beta_{i}^{2r}.$$

Here $A_p$ is a $p \times p$ nonsingular scale matrix, $r$ is a positive integer referred to as the order of the density and $d_p$ is the normalizing constant independent of $\tau$ and $\sigma^2$. Clearly, the density is zero when any component of $\beta$ is zero. The scale parameter $\tau$ is of particular importance, as it reflects the dispersion of the non-local prior density around zero, and implicitly determines the size of the regression coefficients that will be shrunk to zero.

The primary goal and innovation of my research is to propose and analyze a fully Bayesian approach with the pMOM non-local prior where we place an appropriate Inverse-Gamma prior on the parameter $\tau$. A clear advantage of such an approach, compared to previous approaches, is that it is more robust and comparatively immune to misspecification of $\tau$. Under standard regularity assumptions, we show that the posterior probability of the true model converges in probability to $1$ as $n \rightarrow \infty$. Through simulation studies, we demonstrate that our model selection procedure can outperform commonly used penalized likelihood methods and other Bayesian methods which treat the scale parameter as a constant, in a range of simulation settings.

## Statistical applications in neuroscience

Recent studies based on resting fMRI have provided evidence supporting the neurocognitive disturbance of LGG in small-world networks, which is characterized by a high local interconnectedness (high C) and a short path length (low L) called a 'small-world network'. Several studies have shown that both structural and functional brain networks in healthy humans and animals can be characterized by the small-world principle such as Alzheimer's disease (AD), schizophrenia, and brain tumors. Studies in brain tumor patients have shown that these patients display a loss of the small-world configuration and neurocognitive function, and functional connectivity has been associated with neurocognitive deficits in these patients before operation. However, investigations regarding the impact of frontal lobe LGG after tumor resection on small-world network features in the human brain are relatively rare. 

In my collaboration with other physicians from Nanjing Brain Hospital affiliated to Nanjing Medical University, we specifically focused on the frontal lobe LGG patients to directly investigate if these patients exist small-world topological property before and after operation under resting-state, and we expect to find statistical evidence for an alteration of small-world network characteristics in pre-postoperative frontal lobes LGG which may be responsible for the altered neurocognitive function.